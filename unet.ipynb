{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef5f83d-877a-4900-a885-f59e2dbc0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('denoise_dataset.pkl', 'rb') as f:\n",
    "    originals, noisy = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ae8ce4-9410-41da-a7c7-108c7782da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "MODEL_NAME = \"denoiser_unet\"\n",
    "TRAIN_DATA_SIZE = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7edba915-04c5-41ce-9cd8-23526bd35829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla V100-SXM2-16GB, compute capability 7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 14:05:31.623502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-08 14:05:31.631541: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-08 14:05:31.632472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-08 14:05:31.634204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-08 14:05:31.635751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-08 14:05:32.394284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-08 14:05:32.395224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-08 14:05:32.396170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-08 14:05:32.396981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14640 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:03:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((noisy, originals)).shuffle(1028).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "train_size = int(TRAIN_DATA_SIZE * dataset.cardinality().numpy())\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "test_dataset = dataset.skip(train_size)\n",
    "\n",
    "assert dataset.cardinality() == train_dataset.cardinality() + test_dataset.cardinality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4bc566a-4fcb-4602-bf8a-1a4096dd54c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, n_layers, kernel_size, dropout_rate=0.2):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.conv = Conv1D(n_layers, kernel_size, activation='relu', padding='same')\n",
    "        self.conv2 = Conv1D(n_layers*2, kernel_size, activation='relu', padding='same')\n",
    "        self.conv3 = Conv1D(n_layers*2, kernel_size, activation='relu', padding='same')\n",
    "        self.conv4 = Conv1D(n_layers, kernel_size, activation='relu', padding='same')\n",
    "\n",
    "        self.pooling = MaxPool1D(2)\n",
    "\n",
    "        self.bn = BatchNormalization()\n",
    "        self.dropout = SpatialDropout1D(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        skip = self.conv4(x)\n",
    "\n",
    "        x = self.pooling(skip)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x, skip\n",
    "\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, n_layers, kernel_size, dropout_rate=0.2):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.upconv = Conv1DTranspose(n_layers, kernel_size=kernel_size, strides=2, activation='relu', padding='same')\n",
    "        self.conv = Conv1D(n_layers*2, kernel_size=kernel_size, activation='relu', padding='same')\n",
    "        self.conv2 = Conv1D(n_layers, kernel_size, activation='relu', padding='same')\n",
    "        self.bn = BatchNormalization()\n",
    "        self.dropout = SpatialDropout1D(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.upconv(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class Autoencoder(Model):\n",
    "    PAD = 250\n",
    "    GAUSSIAN_STDDEV = 2/5500\n",
    "    SPATIAL_DROPOUT_PROB = 0.2\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.reshape = Reshape([5500, 1])\n",
    "        self.noise = GaussianNoise(stddev=self.GAUSSIAN_STDDEV)\n",
    "        self.normalize = Normalization()\n",
    "        self.pad = ZeroPadding1D(self.PAD)\n",
    "\n",
    "        self.encoder1 = EncoderLayer(512, 3, dropout_rate=self.SPATIAL_DROPOUT_PROB)\n",
    "        self.encoder2 = EncoderLayer(256, 3, dropout_rate=self.SPATIAL_DROPOUT_PROB)\n",
    "        self.encoder3 = EncoderLayer(128, 3, dropout_rate=self.SPATIAL_DROPOUT_PROB)\n",
    "        self.encoder4 = EncoderLayer(64, 3, dropout_rate=self.SPATIAL_DROPOUT_PROB)\n",
    "\n",
    "        self.latent_proj = Sequential([\n",
    "            Conv1D(32, 1, activation='relu', padding='same'),\n",
    "            Conv1D(64, 1, activation='relu', padding='same'),\n",
    "            Conv1D(32, 1, activation='relu', padding='same')\n",
    "        ])\n",
    "\n",
    "        self.decoder1 = DecoderLayer(64, 4, dropout_rate=self.SPATIAL_DROPOUT_PROB)\n",
    "        self.decoder2 = DecoderLayer(128, 2, dropout_rate=self.SPATIAL_DROPOUT_PROB)\n",
    "        self.decoder3 = DecoderLayer(256, 2, dropout_rate=self.SPATIAL_DROPOUT_PROB)\n",
    "        self.decoder4 = DecoderLayer(512, 2, dropout_rate=self.SPATIAL_DROPOUT_PROB)\n",
    "\n",
    "\n",
    "        self.concat1 = Concatenate()\n",
    "        self.concat2 = Concatenate()\n",
    "        self.concat3 = Concatenate()\n",
    "        self.concat4 = Concatenate()\n",
    "\n",
    "        self.upsample = Conv1DTranspose(32, kernel_size=1, strides=2, activation='relu', padding='same')\n",
    "        \n",
    "        self.out = Sequential([\n",
    "            Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "            Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "            Conv1D(32, kernel_size=3, activation='relu', padding='same'),\n",
    "            Conv1D(1, kernel_size=1, activation='tanh', padding='same')\n",
    "        ])\n",
    "\n",
    "        self.crop = Cropping1D(self.PAD)\n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = self.reshape(x) # [5500, 1]\n",
    "        x = self.noise(x)\n",
    "        x = self.normalize(x)\n",
    "\n",
    "        x = self.pad(x) # [6000, 1]\n",
    "\n",
    "        x, skip1 = self.encoder1(x) # [3000, 512], [6000, 512]\n",
    "        x, skip2 = self.encoder2(x) # [1500, 256], [3000, 256]\n",
    "        x, skip3 = self.encoder3(x) # [750, 128], [1500, 128]\n",
    "        x, skip4 = self.encoder4(x) # [375, 64], [750, 64]\n",
    "\n",
    "        latent = self.latent_proj(x) # [375, 32]\n",
    "\n",
    "        x = self.decoder1(latent) # [750, 64]\n",
    "        x = self.concat1([x, skip4]) # [750, 64+64]\n",
    "\n",
    "        x = self.decoder2(x) # [1500, 128]\n",
    "        x = self.concat2([x, skip3]) # [1500, 128+128]\n",
    "\n",
    "        x = self.decoder3(x) # [3000, 256]\n",
    "        x = self.concat3([x, skip2]) # [3000, 256+256]\n",
    "\n",
    "        x = self.decoder4(x) # [6000, 512]\n",
    "        x = self.concat4([x, skip1]) # [6000, 512+512]\n",
    "\n",
    "        x = self.crop(x) # [5500, 1024]\n",
    "        x = self.upsample(x) # [11000, 32]\n",
    "        out = self.out(x) # [11000, 1]\n",
    "\n",
    "        return out\n",
    "# Add data aug\n",
    "# ADD spatial dropout\n",
    "    \n",
    "model = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d766f-6ef4-484a-a68b-75a01ca61e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='mse', \n",
    "            metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, min_delta=0.0001)\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.3, min_lr=1e-6, min_delta=0.005)\n",
    "\n",
    "logdir = os.path.join(f\"{MODEL_NAME}_logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{MODEL_NAME}.tf', \n",
    "                                                         save_format=\"tf\",\n",
    "                                                         monitor='val_loss', save_best_only=True)\n",
    "\n",
    "model.fit(train_dataset,\n",
    "     validation_data=test_dataset,\n",
    "     epochs=25,\n",
    "     callbacks=[early_stopping_callback, checkpoint_callback, reduce_lr_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb0bf5e5-6fb2-492a-be92-e04c23350e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 6s 77ms/step - loss: 0.0059 - mean_absolute_error: 0.0464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005875778384506702, 0.046353068202733994]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0451254-0281-480b-8c27-00a70e457041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('denoise_testset_noisy.pkl', 'rb') as f:\n",
    "    eval_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "415f570d-5362-47a5-a9b0-16f711fc2364",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_pred = model.predict(eval_data).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64d86a3d-e866-4aa6-a5c5-f0cd7229c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(preds):\n",
    "    return \"\\n\".join([ \";\".join([ str(number) for number in p ]) for p in preds.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f740919-2666-464f-9129-158df95a7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answer.txt\", \"w+\") as f:\n",
    "    f.write(format_predictions(eval_data_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_base",
   "language": "python",
   "name": "conda_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
